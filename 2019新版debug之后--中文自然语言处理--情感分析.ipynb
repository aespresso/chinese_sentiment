{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 用Tensorflow进行中文自然语言处理--情感分析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$f('真好喝')=1$$\n",
    "$$f('太难喝了')=0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**简介**  \n",
    "大家好，我是Espresso，这是我制作的第一个教程，是一个简单的中文自然语言处理的分类实践。  \n",
    "制作此教程的目的是什么呢？虽然现在自然语言处理的学习资料很多，英文的资料更多，但是网上资源很乱，尤其是中文的系统的学习资料稀少，而且知识点非常分散，缺少比较系统的实践学习资料，就算有一些代码但因为缺少注释导致要花费很长时间才能理解，我个人在学习过程中，在网络搜索花费了一整天时间，才把处理中文的步骤和需要的软件梳理出来。  \n",
    "所以我觉得自己有义务制作一个入门教程把零散的资料结合成一个实践案例方便各位同学学习，在这个教程中我注重的是实践部分，理论部分我推荐学习deeplearning.ai的课程，在下面的代码部分，涉及到哪方面知识的，我推荐一些学习资料并附上链接，如有侵权请e-mail：a66777@188.com。  \n",
    "另外我对自然语言处理并没有任何深入研究，欢迎各位大牛吐槽，希望能指出不足和改善方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**需要的库**  \n",
    "numpy  \n",
    "jieba  \n",
    "gensim  \n",
    "tensorflow  \n",
    "matplotlib  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 首先加载必用的库\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import jieba # 结巴分词\n",
    "# gensim用来加载预训练word vector\n",
    "from gensim.models import KeyedVectors\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# 用来解压\n",
    "import bz2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**预训练词向量**  \n",
    "本教程使用了北京师范大学中文信息处理研究所与中国人民大学 DBIIR 实验室的研究者开源的\"chinese-word-vectors\" github链接为：  \n",
    "https://github.com/Embedding/Chinese-Word-Vectors  \n",
    "如果你不知道word2vec是什么，我推荐以下一篇文章：  \n",
    "https://zhuanlan.zhihu.com/p/26306795  \n",
    "这里我们使用了\"chinese-word-vectors\"知乎Word + Ngram的词向量，可以从上面github链接下载，我们先加载预训练模型并进行一些简单测试："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 请将下载的词向量压缩包放置在根目录 embeddings 文件夹里\n",
    "# 解压词向量, 有可能需要等待1-2分钟\n",
    "with open(\"embeddings/sgns.zhihu.bigram\", 'wb') as new_file, open(\"embeddings/sgns.zhihu.bigram.bz2\", 'rb') as file:\n",
    "    decompressor = bz2.BZ2Decompressor()\n",
    "    for data in iter(lambda : file.read(100 * 1024), b''):\n",
    "        new_file.write(decompressor.decompress(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用gensim加载预训练中文分词embedding, 有可能需要等待1-2分钟\n",
    "cn_model = KeyedVectors.load_word2vec_format('embeddings/sgns.zhihu.bigram', \n",
    "                                             binary=False, unicode_errors=\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**词向量模型**  \n",
    "在这个词向量模型里，每一个词是一个索引，对应的是一个长度为300的向量，我们今天需要构建的LSTM神经网络模型并不能直接处理汉字文本，需要先进行分次并把词汇转换为词向量，步骤请参考下图，步骤的讲解会跟着代码一步一步来，如果你不知道RNN，GRU，LSTM是什么，我推荐deeplearning.ai的课程，网易公开课有免费中文字幕版，但我还是推荐有习题和练习代码部分的的coursera原版：  \n",
    "<img src='flowchart.jpg' style='width:400px;'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "× 0.6080275177955627\n",
      "*( 0.5801156163215637\n",
      "%& 0.5629096031188965\n",
      "Sf 0.5612465143203735\n",
      ")⃕↝ 0.5575112104415894\n",
      "VXrain 0.551669716835022\n",
      "VYrain 0.5468728542327881\n",
      "Sbh 0.5460761785507202\n",
      "Vr 0.5429755449295044\n",
      "Sab 0.5415331125259399\n",
      "词向量的长度为300\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-2.603470e-01,  3.677500e-01, -2.379650e-01,  5.301700e-02,\n",
       "       -3.628220e-01, -3.212010e-01, -1.903330e-01,  1.587220e-01,\n",
       "       -7.156200e-02, -4.625400e-02, -1.137860e-01,  3.515600e-01,\n",
       "       -6.408200e-02, -2.184840e-01,  3.286950e-01, -7.110330e-01,\n",
       "        1.620320e-01,  1.627490e-01,  5.528180e-01,  1.016860e-01,\n",
       "        1.060080e-01,  7.820700e-01, -7.537310e-01, -2.108400e-02,\n",
       "       -4.758250e-01, -1.130420e-01, -2.053000e-01,  6.624390e-01,\n",
       "        2.435850e-01,  9.171890e-01, -2.090610e-01, -5.290000e-02,\n",
       "       -7.969340e-01,  2.394940e-01, -9.028100e-02,  1.537360e-01,\n",
       "       -4.003980e-01, -2.456100e-02, -1.717860e-01,  2.037790e-01,\n",
       "       -4.344710e-01, -3.850430e-01, -9.366000e-02,  3.775310e-01,\n",
       "        2.659690e-01,  8.879800e-02,  2.493440e-01,  4.914900e-02,\n",
       "        5.996000e-03,  3.586430e-01, -1.044960e-01, -5.838460e-01,\n",
       "        3.093280e-01, -2.828090e-01, -8.563400e-02, -5.745400e-02,\n",
       "       -2.075230e-01,  2.845980e-01,  1.414760e-01,  1.678570e-01,\n",
       "        1.957560e-01,  7.782140e-01, -2.359000e-01, -6.833100e-02,\n",
       "        2.560170e-01, -6.906900e-02, -1.219620e-01,  2.683020e-01,\n",
       "        1.678810e-01,  2.068910e-01,  1.987520e-01,  6.720900e-02,\n",
       "       -3.975290e-01, -7.123140e-01,  5.613200e-02,  2.586000e-03,\n",
       "        5.616910e-01,  1.157000e-03, -4.341190e-01,  1.977480e-01,\n",
       "        2.519540e-01,  8.835000e-03, -3.554600e-01, -1.573500e-02,\n",
       "       -2.526010e-01,  9.355900e-02, -3.962500e-02, -1.628350e-01,\n",
       "        2.980950e-01,  1.647900e-01, -5.454270e-01,  3.888790e-01,\n",
       "        1.446840e-01, -7.239600e-02, -7.597800e-02, -7.803000e-03,\n",
       "        2.020520e-01, -4.424750e-01,  3.911580e-01,  2.115100e-01,\n",
       "        6.516760e-01,  5.668030e-01,  5.065500e-02, -1.259650e-01,\n",
       "       -3.720640e-01,  2.330470e-01,  6.659900e-02,  8.300600e-02,\n",
       "        2.540460e-01, -5.279760e-01, -3.843280e-01,  3.366460e-01,\n",
       "        2.336500e-01,  3.564750e-01, -4.884160e-01, -1.183910e-01,\n",
       "        1.365910e-01,  2.293420e-01, -6.151930e-01,  5.212050e-01,\n",
       "        3.412000e-01,  5.757940e-01,  2.354480e-01, -3.641530e-01,\n",
       "        7.373400e-02,  1.007380e-01, -3.211410e-01, -3.040480e-01,\n",
       "       -3.738440e-01, -2.515150e-01,  2.633890e-01,  3.995490e-01,\n",
       "        4.461880e-01,  1.641110e-01,  1.449590e-01, -4.191540e-01,\n",
       "        2.297840e-01,  6.710600e-02,  3.316430e-01, -6.026500e-02,\n",
       "       -5.130610e-01,  1.472570e-01,  2.414060e-01,  2.011000e-03,\n",
       "       -3.823410e-01, -1.356010e-01,  3.112300e-01,  9.177830e-01,\n",
       "       -4.511630e-01,  1.272190e-01, -9.431600e-02, -8.216000e-03,\n",
       "       -3.835440e-01,  2.589400e-02,  6.374980e-01,  4.931630e-01,\n",
       "       -1.865070e-01,  4.076900e-01, -1.841000e-03,  2.213160e-01,\n",
       "        2.253950e-01, -2.159220e-01, -7.611480e-01, -2.305920e-01,\n",
       "        1.296890e-01, -1.304100e-01, -4.742270e-01,  2.275500e-02,\n",
       "        4.255050e-01,  1.570280e-01,  2.975300e-02,  1.931830e-01,\n",
       "        1.304340e-01, -3.179800e-02,  1.516650e-01, -2.154310e-01,\n",
       "       -4.681410e-01,  1.007326e+00, -6.698940e-01, -1.555240e-01,\n",
       "        1.797170e-01,  2.848660e-01,  6.216130e-01,  1.549510e-01,\n",
       "        6.225000e-02, -2.227800e-02,  2.561270e-01, -1.006380e-01,\n",
       "        2.807900e-02,  4.597710e-01, -4.077750e-01, -1.777390e-01,\n",
       "        1.920500e-02, -4.829300e-02,  4.714700e-02, -3.715200e-01,\n",
       "       -2.995930e-01, -3.719710e-01,  4.622800e-02, -1.436460e-01,\n",
       "        2.532540e-01, -9.334000e-02, -4.957400e-02, -3.803850e-01,\n",
       "        5.970110e-01,  3.578450e-01, -6.826000e-02,  4.735200e-02,\n",
       "       -3.707590e-01, -8.621300e-02, -2.556480e-01, -5.950440e-01,\n",
       "       -4.757790e-01,  1.079320e-01,  9.858300e-02,  8.540300e-01,\n",
       "        3.518370e-01, -1.306360e-01, -1.541590e-01,  1.166775e+00,\n",
       "        2.048860e-01,  5.952340e-01,  1.158830e-01,  6.774400e-02,\n",
       "        6.793920e-01, -3.610700e-01,  1.697870e-01,  4.118530e-01,\n",
       "        4.731000e-03, -7.516530e-01, -9.833700e-02, -2.312220e-01,\n",
       "       -7.043300e-02,  1.576110e-01, -4.780500e-02, -7.344390e-01,\n",
       "       -2.834330e-01,  4.582690e-01,  3.957010e-01, -8.484300e-02,\n",
       "       -3.472550e-01,  1.291660e-01,  3.838960e-01, -3.287600e-02,\n",
       "       -2.802220e-01,  5.257030e-01, -3.609300e-02, -4.842220e-01,\n",
       "        3.690700e-02,  3.429560e-01,  2.902490e-01, -1.624650e-01,\n",
       "       -7.513700e-02,  2.669300e-01,  5.778230e-01, -3.074020e-01,\n",
       "       -2.183790e-01, -2.834050e-01,  1.350870e-01,  1.490070e-01,\n",
       "        1.438400e-02, -2.509040e-01, -3.376100e-01,  1.291880e-01,\n",
       "       -3.808700e-01, -4.420520e-01, -2.512300e-01, -1.328990e-01,\n",
       "       -1.211970e-01,  2.532660e-01,  2.757050e-01, -3.382040e-01,\n",
       "        1.178070e-01,  3.860190e-01,  5.277960e-01,  4.581920e-01,\n",
       "        1.502310e-01,  1.226320e-01,  2.768540e-01, -4.502080e-01,\n",
       "       -1.992670e-01,  1.689100e-02,  1.188860e-01,  3.502440e-01,\n",
       "       -4.064770e-01,  2.610280e-01, -1.934990e-01, -1.625660e-01,\n",
       "        2.498400e-02, -1.867150e-01, -1.954400e-02, -2.281900e-01,\n",
       "       -3.417670e-01, -5.222770e-01, -9.543200e-02, -3.500350e-01,\n",
       "        2.154600e-02,  2.318040e-01,  5.395310e-01, -4.223720e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for e in cn_model.most_similar(positive=['*'], topn=10):\n",
    "   print(e[0], e[1])\n",
    "# 由此可见每一个词都对应一个长度为300的向量\n",
    "embedding_dim = cn_model['山东大学'].shape[0]\n",
    "print('词向量的长度为{}'.format(embedding_dim))\n",
    "cn_model['山东大学']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cosine Similarity for Vector Space Models by Christian S. Perone\n",
    "http://blog.christianperone.com/2013/09/machine-learning-cosine-similarity-for-vector-space-models-part-iii/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6612811836240267"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 计算相似度\n",
    "cn_model.similarity('橘子', '橙子')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.66128117"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dot（'橘子'/|'橘子'|， '橙子'/|'橙子'| ）\n",
    "np.dot(cn_model['橘子']/np.linalg.norm(cn_model['橘子']), \n",
    "cn_model['橙子']/np.linalg.norm(cn_model['橙子']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('高中', 0.7247823476791382),\n",
       " ('本科', 0.6768535375595093),\n",
       " ('研究生', 0.6244412660598755),\n",
       " ('中学', 0.6088204979896545),\n",
       " ('大学本科', 0.595908522605896),\n",
       " ('初中', 0.5883588790893555),\n",
       " ('读研', 0.5778335332870483),\n",
       " ('职高', 0.5767995119094849),\n",
       " ('大学毕业', 0.5767451524734497),\n",
       " ('师范大学', 0.5708829760551453)]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 找出最相近的词，余弦相似度\n",
    "cn_model.most_similar(positive=['大学'], topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "在 老师 会计师 程序员 律师 医生 老人 中:\n",
      "不是同一类别的词为: 老人\n"
     ]
    }
   ],
   "source": [
    "# 找出不同的词\n",
    "test_words = '老师 会计师 程序员 律师 医生 老人'\n",
    "test_words_result = cn_model.doesnt_match(test_words.split())\n",
    "print('在 '+test_words+' 中:\\n不是同一类别的词为: %s' %test_words_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('出轨', 0.6100173592567444)]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cn_model.most_similar(positive=['女人','劈腿'], negative=['男人'], topn=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**训练语料**  \n",
    "本教程使用了谭松波老师的酒店评论语料，即使是这个语料也很难找到下载链接，在某博客还得花积分下载，而我不知道怎么赚取积分，后来好不容易找到一个链接但竟然是失效的，再后来尝试把链接粘贴到迅雷上终于下载了下来，希望大家以后多多分享资源。  \n",
    "训练样本分别被放置在两个文件夹里：\n",
    "分别的pos和neg，每个文件夹里有2000个txt文件，每个文件内有一段评语，共有4000个训练样本，这样大小的样本数据在NLP中属于非常迷你的："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获得样本的索引，样本存放于两个文件夹中，\n",
    "# 分别为 正面评价'pos'文件夹 和 负面评价'neg'文件夹\n",
    "# 每个文件夹中有2000个txt文件，每个文件中是一例评价\n",
    "import os\n",
    "pos_txts = os.listdir('pos')\n",
    "neg_txts = os.listdir('neg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "样本总共: 199\n"
     ]
    }
   ],
   "source": [
    "print( '样本总共: '+ str(len(pos_txts) + len(neg_txts)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 现在我们将所有的评价内容放置到一个list里\n",
    "# 这里和视频课程不一样, 因为很多同学反应按照视频课程里的读取方法会乱码,\n",
    "# 经过检查发现是原始文本里的编码是gbk造成的,\n",
    "# 这里我进行了简单的预处理, 以避免乱码\n",
    "train_texts_orig = []\n",
    "# 文本所对应的labels, 也就是标记\n",
    "train_target = []\n",
    "with open(\"positive_samples.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        dic = eval(line)\n",
    "        train_texts_orig.append(dic[\"text\"])\n",
    "        train_target.append(dic[\"label\"])\n",
    "\n",
    "with open(\"negative_samples.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        dic = eval(line)\n",
    "        train_texts_orig.append(dic[\"text\"])\n",
    "        train_target.append(dic[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4000"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_texts_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 我们使用tensorflow的keras接口来建模\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Dense, GRU, Embedding, LSTM, Bidirectional\n",
    "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.python.keras.optimizers import RMSprop\n",
    "from tensorflow.python.keras.optimizers import Adam\n",
    "from tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**分词和tokenize**  \n",
    "首先我们去掉每个样本的标点符号，然后用jieba分词，jieba分词返回一个生成器，没法直接进行tokenize，所以我们将分词结果转换成一个list，并将它索引化，这样每一例评价的文本变成一段索引数字，对应着预训练词向量模型中的词。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 进行分词和tokenize\n",
    "# train_tokens是一个长长的list，其中含有4000个小list，对应每一条评价\n",
    "train_tokens = []\n",
    "for text in train_texts_orig:\n",
    "    # 去掉标点\n",
    "    text = re.sub(\"[\\s+\\.\\!\\/_,$%^*(+\\\"\\']+|[+——！，。？、~@#￥%……&*（）]+\", \"\",text)\n",
    "    # 结巴分词\n",
    "    cut = jieba.cut(text)\n",
    "    # 结巴分词的输出结果为一个生成器\n",
    "    # 把生成器转换为list\n",
    "    cut_list = [ i for i in cut ]\n",
    "    for i, word in enumerate(cut_list):\n",
    "        try:\n",
    "            # 将词转换为索引index\n",
    "            cut_list[i] = cn_model.vocab[word].index\n",
    "        except KeyError:\n",
    "            # 如果词不在字典中，则输出0\n",
    "            cut_list[i] = cn_model.vocab['*'].index\n",
    "    train_tokens.append(cut_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**索引长度标准化**  \n",
    "因为每段评语的长度是不一样的，我们如果单纯取最长的一个评语，并把其他评填充成同样的长度，这样十分浪费计算资源，所以我们取一个折衷的长度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获得所有tokens的长度\n",
    "num_tokens = [ len(tokens) for tokens in train_tokens ]\n",
    "num_tokens = np.array(num_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "71.4495"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 平均tokens的长度\n",
    "np.mean(num_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1540"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 最长的评价tokens的长度\n",
    "np.max(num_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(np.log(num_tokens), bins = 100)\n",
    "plt.xlim((0,10))\n",
    "plt.ylabel('number of tokens')\n",
    "plt.xlabel('length of tokens')\n",
    "plt.title('Distribution of tokens length')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "236"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 取tokens平均值并加上两个tokens的标准差，\n",
    "# 假设tokens长度的分布为正态分布，则max_tokens这个值可以涵盖95%左右的样本\n",
    "max_tokens = np.mean(num_tokens) + 2 * np.std(num_tokens)\n",
    "max_tokens = int(max_tokens)\n",
    "max_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9565"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 取tokens的长度为236时，大约95%的样本被涵盖\n",
    "# 我们对长度不足的进行padding，超长的进行修剪\n",
    "np.sum( num_tokens < max_tokens ) / len(num_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**反向tokenize**  \n",
    "我们定义一个function，用来把索引转换成可阅读的文本，这对于debug很重要。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用来将tokens转换为文本\n",
    "def reverse_tokens(tokens):\n",
    "    text = ''\n",
    "    for i in tokens:\n",
    "        if i != 0:\n",
    "            text = text + cn_model.index2word[i]\n",
    "        else:\n",
    "            text = text + ' '\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse = reverse_tokens(train_tokens[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下可见，训练样本的极性并不是那么精准，比如说下面的样本，对早餐并不满意，但被定义为正面评价，这会迷惑我们的模型，不过我们暂时不对训练样本进行任何修改。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'早餐太差无论去多少人那边也不加食品的酒店应该重视一下这个问题了房间本身很好'"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 经过tokenize再恢复成文本\n",
    "# 可见标点符号都没有了\n",
    "reverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'早餐太差，无论去多少人，那边也不加食品的。酒店应该重视一下这个问题了。\\n\\n房间本身很好。'"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 原始文本\n",
    "train_texts_orig[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**准备Embedding Matrix**  \n",
    "现在我们来为模型准备embedding matrix（词向量矩阵），根据keras的要求，我们需要准备一个维度为$(numwords, embeddingdim)$的矩阵，num words代表我们使用的词汇的数量，emdedding dimension在我们现在使用的预训练词向量模型中是300，每一个词汇都用一个长度为300的向量表示。  \n",
    "注意我们只选择使用前50k个使用频率最高的词，在这个预训练词向量模型中，一共有260万词汇量，如果全部使用在分类问题上会很浪费计算资源，因为我们的训练样本很小，一共只有4k，如果我们有100k，200k甚至更多的训练样本时，在分类问题上可以考虑减少使用的词汇量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 只使用前20000个词\n",
    "num_words = 50000\n",
    "# 初始化embedding_matrix，之后在keras上进行应用\n",
    "embedding_matrix = np.zeros((num_words, embedding_dim))\n",
    "# embedding_matrix为一个 [num_words，embedding_dim] 的矩阵\n",
    "# 维度为 50000 * 300\n",
    "for i in range(num_words):\n",
    "    embedding_matrix[i,:] = cn_model[cn_model.index2word[i]]\n",
    "embedding_matrix = embedding_matrix.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 检查index是否对应，\n",
    "# 输出300意义为长度为300的embedding向量一一对应\n",
    "np.sum( cn_model[cn_model.index2word[333]] == embedding_matrix[333] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 300)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# embedding_matrix的维度，\n",
    "# 这个维度为keras的要求，后续会在模型中用到\n",
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**padding（填充）和truncating（修剪）**  \n",
    "我们把文本转换为tokens（索引）之后，每一串索引的长度并不相等，所以为了方便模型的训练我们需要把索引的长度标准化，上面我们选择了236这个可以涵盖95%训练样本的长度，接下来我们进行padding和truncating，我们一般采用'pre'的方法，这会在文本索引的前面填充0，因为根据一些研究资料中的实践，如果在文本索引后面填充0的话，会对模型造成一些不良影响。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 进行padding和truncating， 输入的train_tokens是一个list\n",
    "# 返回的train_pad是一个numpy array\n",
    "train_pad = pad_sequences(train_tokens, maxlen=max_tokens,\n",
    "                            padding='pre', truncating='pre')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1659\n"
     ]
    }
   ],
   "source": [
    "# 超出五万个词向量的词用0代替\n",
    "print(cn_model.vocab['*'].index)\n",
    "train_pad[ train_pad>=num_words ] = cn_model.vocab['*'].index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 可见padding之后前面的tokens全变成0，文本在最后面\n",
    "train_pad[33]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 准备target向量，前2000样本为1，后2000为0\n",
    "train_target = np.array(train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 进行训练和测试样本的分割\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 90%的样本用来训练，剩余10%用来测试\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_pad,\n",
    "                                                    train_target,\n",
    "                                                    test_size=0.1,\n",
    "                                                    random_state=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                                                        房间很大还有海景阳台走出酒店就是沙滩非常不错唯一遗憾的就是不能刷 不方便\n",
      "class:  1\n"
     ]
    }
   ],
   "source": [
    "# 查看训练样本，确认无误\n",
    "print(reverse_tokens(X_train[35]))\n",
    "print('class: ',y_train[35])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在我们用keras搭建LSTM模型，模型的第一层是Embedding层，只有当我们把tokens索引转换为词向量矩阵之后，才可以用神经网络对文本进行处理。\n",
    "keras提供了Embedding接口，避免了繁琐的稀疏矩阵操作。   \n",
    "在Embedding层我们输入的矩阵为：$$(batchsize, maxtokens)$$\n",
    "输出矩阵为： $$(batchsize, maxtokens, embeddingdim)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用LSTM对样本进行分类\n",
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型第一层为embedding\n",
    "model.add(Embedding(num_words,\n",
    "                    embedding_dim,\n",
    "                    weights=[embedding_matrix],\n",
    "                    input_length=max_tokens,\n",
    "                    trainable=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在2019年6月10日修改了一些大坑的bug, 可能是数据的顺序变了, \n",
    "# 结果模型训练的效果没有去年最早的时候效果好了, \n",
    "# 有兴趣的同学可以调整一下模型参数, 看看会不会有更好的结果\n",
    "model.add(LSTM (units=128, return_sequences = True))  \n",
    "    # Add dropout with a probability of 0.5\n",
    "#model.add(Dropout(0.5))\n",
    "model.add(Bidirectional(LSTM(units=128, return_sequences=True)))\n",
    "model.add(LSTM(units=16, return_sequences=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**构建模型**  \n",
    "我在这个教程中尝试了几种神经网络结构，因为训练样本比较少，所以我们可以尽情尝试，训练过程等待时间并不长：  \n",
    "**GRU：**如果使用GRU的话，测试样本可以达到87%的准确率，但我测试自己的文本内容时发现，GRU最后一层激活函数的输出都在0.5左右，说明模型的判断不是很明确，信心比较低，而且经过测试发现模型对于否定句的判断有时会失误，我们期望对于负面样本输出接近0，正面样本接近1而不是都徘徊于0.5之间。  \n",
    "**BiLSTM：**测试了LSTM和BiLSTM，发现BiLSTM的表现最好，LSTM的表现略好于GRU，这可能是因为BiLSTM对于比较长的句子结构有更好的记忆，有兴趣的朋友可以深入研究一下。  \n",
    "Embedding之后第，一层我们用BiLSTM返回sequences，然后第二层16个单元的LSTM不返回sequences，只返回最终结果，最后是一个全链接层，用sigmoid激活函数输出结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRU的代码\n",
    "# model.add(GRU(units=32, return_sequences=True))\n",
    "# model.add(GRU(units=16, return_sequences=True))\n",
    "# model.add(GRU(units=4, return_sequences=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(1, activation='sigmoid'))\n",
    "# 我们使用adam以0.001的learning rate进行优化\n",
    "optimizer = Adam(lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimizer,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_7 (Embedding)      (None, 236, 300)          15000000  \n",
      "_________________________________________________________________\n",
      "lstm_26 (LSTM)               (None, 236, 128)          219648    \n",
      "_________________________________________________________________\n",
      "bidirectional_11 (Bidirectio (None, 236, 256)          263168    \n",
      "_________________________________________________________________\n",
      "lstm_28 (LSTM)               (None, 16)                17472     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 15,500,305\n",
      "Trainable params: 500,305\n",
      "Non-trainable params: 15,000,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 我们来看一下模型的结构，一共90k左右可训练的变量\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建立一个权重的存储点\n",
    "path_checkpoint = 'sentiment_checkpoint.keras'\n",
    "checkpoint = ModelCheckpoint(filepath=path_checkpoint, monitor='val_loss',\n",
    "                                      verbose=1, save_weights_only=True,\n",
    "                                      save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are trying to load a weight file containing 6 layers into a model with 5 layers.\n"
     ]
    }
   ],
   "source": [
    "# 尝试加载已训练模型\n",
    "try:\n",
    "    model.load_weights(path_checkpoint)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义early stoping如果3个epoch内validation loss没有改善则停止训练\n",
    "earlystopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自动降低learning rate\n",
    "lr_reduction = ReduceLROnPlateau(monitor='val_loss',\n",
    "                                       factor=0.1, min_lr=1e-8, patience=0,\n",
    "                                       verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义callback函数\n",
    "callbacks = [\n",
    "    earlystopping, \n",
    "    checkpoint,\n",
    "    lr_reduction\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3240 samples, validate on 360 samples\n",
      "Epoch 1/20\n",
      "3240/3240 [==============================] - 43s 13ms/step - loss: 0.5710 - acc: 0.6957 - val_loss: 0.4412 - val_acc: 0.8111\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.44117, saving model to sentiment_checkpoint.keras\n",
      "Epoch 2/20\n",
      "3240/3240 [==============================] - 39s 12ms/step - loss: 0.4188 - acc: 0.8225 - val_loss: 0.3941 - val_acc: 0.8417\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.44117 to 0.39407, saving model to sentiment_checkpoint.keras\n",
      "Epoch 3/20\n",
      "3240/3240 [==============================] - 39s 12ms/step - loss: 0.4546 - acc: 0.8022 - val_loss: 0.3977 - val_acc: 0.8361\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.39407\n",
      "\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 4/20\n",
      "3240/3240 [==============================] - 39s 12ms/step - loss: 0.3592 - acc: 0.8614 - val_loss: 0.3759 - val_acc: 0.8528\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.39407 to 0.37591, saving model to sentiment_checkpoint.keras\n",
      "Epoch 5/20\n",
      "3240/3240 [==============================] - 39s 12ms/step - loss: 0.3355 - acc: 0.8679 - val_loss: 0.3595 - val_acc: 0.8639\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.37591 to 0.35946, saving model to sentiment_checkpoint.keras\n",
      "Epoch 6/20\n",
      "3240/3240 [==============================] - 39s 12ms/step - loss: 0.3217 - acc: 0.8719 - val_loss: 0.3540 - val_acc: 0.8639\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.35946 to 0.35402, saving model to sentiment_checkpoint.keras\n",
      "Epoch 7/20\n",
      "3240/3240 [==============================] - 39s 12ms/step - loss: 0.3098 - acc: 0.8796 - val_loss: 0.3483 - val_acc: 0.8611\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.35402 to 0.34831, saving model to sentiment_checkpoint.keras\n",
      "Epoch 8/20\n",
      "3240/3240 [==============================] - 39s 12ms/step - loss: 0.3127 - acc: 0.8753 - val_loss: 0.3619 - val_acc: 0.8500\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.34831\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 9/20\n",
      "3240/3240 [==============================] - 39s 12ms/step - loss: 0.3003 - acc: 0.8799 - val_loss: 0.3525 - val_acc: 0.8611\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.34831\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 10/20\n",
      "3240/3240 [==============================] - 39s 12ms/step - loss: 0.2960 - acc: 0.8843 - val_loss: 0.3522 - val_acc: 0.8611\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.34831\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "Epoch 11/20\n",
      "3240/3240 [==============================] - 39s 12ms/step - loss: 0.2959 - acc: 0.8843 - val_loss: 0.3522 - val_acc: 0.8611\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.34831\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-08.\n",
      "Epoch 12/20\n",
      "3240/3240 [==============================] - 39s 12ms/step - loss: 0.2958 - acc: 0.8843 - val_loss: 0.3522 - val_acc: 0.8611\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.34831\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 1e-08.\n",
      "Epoch 00012: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f299681c4e0>"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 开始训练\n",
    "model.fit(X_train, y_train,\n",
    "          validation_split=0.1, \n",
    "          epochs=20,\n",
    "          batch_size=128,\n",
    "          callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**结论**  \n",
    "我们首先对测试样本进行预测，得到了还算满意的准确度。  \n",
    "之后我们定义一个预测函数，来预测输入的文本的极性，可见模型对于否定句和一些简单的逻辑结构都可以进行准确的判断。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400/400 [==============================] - 2s 5ms/step\n",
      "Accuracy:87.50%\n"
     ]
    }
   ],
   "source": [
    "result = model.evaluate(X_test, y_test)\n",
    "print('Accuracy:{0:.2%}'.format(result[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(text):\n",
    "    print(text)\n",
    "    # 去标点\n",
    "    text = re.sub(\"[\\s+\\.\\!\\/_,$%^*(+\\\"\\']+|[+——！，。？、~@#￥%……&*（）]+\", \"\",text)\n",
    "    # 分词\n",
    "    cut = jieba.cut(text)\n",
    "    cut_list = [ i for i in cut ]\n",
    "    # tokenize\n",
    "    for i, word in enumerate(cut_list):\n",
    "        try:\n",
    "            cut_list[i] = cn_model.vocab[word].index\n",
    "            if cut_list[i] >= 50000:\n",
    "                cut_list[i] = 0\n",
    "        except KeyError:\n",
    "            cut_list[i] = 0\n",
    "    # padding\n",
    "    tokens_pad = pad_sequences([cut_list], maxlen=max_tokens,\n",
    "                           padding='pre', truncating='pre')\n",
    "    # 预测\n",
    "    result = model.predict(x=tokens_pad)\n",
    "    coef = result[0][0]\n",
    "    if coef >= 0.5:\n",
    "        print('是一例正面评价','output=%.2f'%coef)\n",
    "    else:\n",
    "        print('是一例负面评价','output=%.2f'%coef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "酒店设施不是新的，服务态度很不好\n",
      "是一例负面评价 output=0.41\n",
      "酒店卫生条件非常不好\n",
      "是一例负面评价 output=0.36\n",
      "床铺非常舒适\n",
      "是一例正面评价 output=0.64\n",
      "房间很凉，不给开暖气\n",
      "是一例负面评价 output=0.33\n",
      "房间很凉爽，空调冷气很足\n",
      "是一例负面评价 output=0.50\n",
      "酒店环境不好，住宿体验很不好\n",
      "是一例负面评价 output=0.21\n",
      "房间隔音不到位\n",
      "是一例负面评价 output=0.34\n",
      "晚上回来发现没有打扫卫生\n",
      "是一例负面评价 output=0.45\n",
      "因为过节所以要我临时加钱，比团购的价格贵\n",
      "是一例负面评价 output=0.24\n"
     ]
    }
   ],
   "source": [
    "test_list = [\n",
    "    '酒店设施不是新的，服务态度很不好',\n",
    "    '酒店卫生条件非常不好',\n",
    "    '床铺非常舒适',\n",
    "    '房间很凉，不给开暖气',\n",
    "    '房间很凉爽，空调冷气很足',\n",
    "    '酒店环境不好，住宿体验很不好',\n",
    "    '房间隔音不到位' ,\n",
    "    '晚上回来发现没有打扫卫生',\n",
    "    '因为过节所以要我临时加钱，比团购的价格贵'\n",
    "]\n",
    "for text in test_list:\n",
    "    predict_sentiment(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**错误分类的文本**\n",
    "经过查看，发现错误分类的文本的含义大多比较含糊，就算人类也不容易判断极性，如index为101的这个句子，好像没有一点满意的成分，但这例子评价在训练样本中被标记成为了正面评价，而我们的模型做出的负面评价的预测似乎是合理的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "y_pred = y_pred.T[0]\n",
    "y_pred = [1 if p>= 0.5 else 0 for p in y_pred]\n",
    "y_pred = np.array(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_actual = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 找出错误分类的索引\n",
    "misclassified = np.where( y_pred != y_actual )[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400\n"
     ]
    }
   ],
   "source": [
    "# 输出所有错误分类的索引\n",
    "len(misclassified)\n",
    "print(len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                        由于2007年 有一些新问题可能还没来得及解决我因为工作需要经常要住那里所以慎重的提出以下 ：1 后*的 淋浴喷头的位置都太高我换了房间还是一样很不好用2 后的一些管理和服务还很不到位尤其是前台入住和 时代效率太低每次 都超过10分钟好像不符合*宾馆的要求\n",
      "预测的分类 1\n",
      "实际的分类 1\n"
     ]
    }
   ],
   "source": [
    "# 我们来找出错误分类的样本看看\n",
    "idx=101\n",
    "print(reverse_tokens(X_test[idx]))\n",
    "print('预测的分类', y_pred[idx])\n",
    "print('实际的分类', y_actual[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                                                 还是很 设施也不错但是 和以前*比急剧下滑了 和客房 的服务极差幸好我不是很在乎\n",
      "预测的分类 1\n",
      "实际的分类 1\n"
     ]
    }
   ],
   "source": [
    "idx=1\n",
    "print(reverse_tokens(X_test[idx]))\n",
    "print('预测的分类', y_pred[idx])\n",
    "print('实际的分类', y_actual[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
